{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring NLP for Clinical Trial Records\n",
    "## Installment 2: Extracting Temportal Information\n",
    "### by Munif Mujib\n",
    "\n",
    "In this notebook, we'll explore the use of a syntactic parsing-based strategy to extract information that is potentially about scheduling for patients from unstructured trial description text. Then, we'll explore a more lightweight exercise in extracting the potential duration of a trial without leveraging grammar and syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting tools in place\n",
    "\n",
    "First, we'll import the necessary modules and load the previously developed tools for accessing the trial records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import xml.etree.ElementTree as ET\n",
    "import re, glob, time\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "import random\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helpful module we'll be using for grammar and syntax is `spacy`. We'll need to load `spacy`'s English language engine to leverage its capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll need the functions defined in the last installment to access the trial records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_node(parent, parent_dict):\n",
    "    if list(parent):\n",
    "        for child in parent:\n",
    "            child_dict = {}\n",
    "            child_dict = process_node(child, child_dict)\n",
    "            parent_dict[child.tag] = child_dict\n",
    "    else:\n",
    "        parent_dict[\"field_value\"] = parent.text\n",
    "    return parent_dict\n",
    "\n",
    "def load_trial(trial_id, root_dir = \"../clinicaltrials_data/trials/\"):\n",
    "    directory = trial_id[:-4] + \"xxxx\"\n",
    "    filepath = root_dir + directory + \"/\" + trial_id + \".xml\"\n",
    "    \n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    root_dict = {}\n",
    "    return process_node(root, root_dict)\n",
    "\n",
    "def find_textblocks(field, textblocks = [], current = \"root\"):\n",
    "    subfields = field.keys()\n",
    "    if not subfields == [\"field_value\"]:\n",
    "        if \"textblock\" in subfields:\n",
    "            textblocks.append((current, field[\"textblock\"][\"field_value\"]))\n",
    "        if len(subfields) > 1:    \n",
    "            for subfield in subfields:\n",
    "                if not subfield == \"textblock\":\n",
    "                    textblocks = find_textblocks(field[subfield], textblocks, current = current + \">\" + subfield)\n",
    "    return textblocks\n",
    "\n",
    "def process_textblock(textblock):\n",
    "    utext = textblock\n",
    "    lines = re.sub(r\"([^\\n.!?\\\"])\\n\\n\",r\"\\1.\\n\\n\",utext)\n",
    "    lines = re.sub(\"\\n*[ ]+\", \" \", lines)\n",
    "    lines = lines.strip()\n",
    "    return lines\n",
    "\n",
    "def get_textblocks(textblocks):\n",
    "    textblocks_dict = {}\n",
    "    for textblock in textblocks:\n",
    "        levels = re.split(\">\", textblock[0])\n",
    "        key = \">\".join(levels[1:])\n",
    "        textblocks_dict[key] = process_textblock(textblock[1])\n",
    "    return textblocks_dict\n",
    "\n",
    "def extract_textblocks(trial_id, root_dir = \"../clinicaltrials_data/trials/\"):\n",
    "    return get_textblocks(find_textblocks(load_trial(trial_id, root_dir = root_dir), textblocks = []))\n",
    "\n",
    "def create_regex_format(x):\n",
    "    regex = re.compile(r\"[^a-z0-9](\" + x + r\")[^a-z0-9]\")\n",
    "    return regex\n",
    "    \n",
    "def find_indicators(element, indicators_dict = {}, current = \"root\"):\n",
    "    if type(element) == dict:\n",
    "        subelements = element.keys()\n",
    "        if \"indicators\" in subelements and element[\"indicators\"]:\n",
    "            indicators_dict[re.sub(\"refinements>\", \"\",current)] = (map(create_regex_format, element[\"indicators\"]))\n",
    "        if len(subelements) > 1:\n",
    "            for subelement in subelements:\n",
    "                if type(element[subelement]) == dict:\n",
    "                    indicators_dict = find_indicators(\n",
    "                        element[subelement], indicators_dict, current = current + \">\" + subelement\n",
    "                    )\n",
    "    return indicators_dict\n",
    "\n",
    "def match_indicators(d, indicators_dict, secID):\n",
    "    lines = d.get(secID,\"\")\n",
    "    found = defaultdict(list)\n",
    "    for sentNum, sentence in enumerate(nltk.sent_tokenize(lines)):\n",
    "        for group in indicators_dict:\n",
    "            matches = []\n",
    "            for n, indicator in enumerate(indicators_dict[group]):\n",
    "                padded = \" \" + sentence.lower().strip() + \" \"\n",
    "                if indicator.search(padded):\n",
    "                    nuggets = indicator.findall(padded)\n",
    "                    matches.append(re.sub(r\"^.*?\\]\\((.*?)\\)\\[.*?$\",r\"\\1\",indicator.pattern))\n",
    "            if matches:\n",
    "                found[group].append((matches, str(sentNum)))\n",
    "    return found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a handy index of sentences that match with the different indicators we've defined and put it in a `json` file. We'll load this \"compound\" index (containing trial IDs, section IDs, and sentence numbers) from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounded = json.load(open(\"../compounded.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a function that can return a collection of `n` sentences matching the criteria for group, indicator, and section. The default indicator (which, frankly, looks quite scary at this point) is the combination of all temporal patterns, capable of picking out mentions of minutes, hours, days, weeks, months, and years. We can modify the `seed` parameter to get the function to retrieve different sets of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_sentences(group = \"root>burden\", \n",
    "                       indicator = \"(?:(?:minute|min\\\\.?|hour|hr\\\\.?|day|week|wk\\\\.?|month|mo\\\\.?|year|yr\\\\.?)s? (?:\\\\d+-|\\\\d+\\\\.)*\\\\d+|(?:\\\\d+-|\\\\d+\\\\.)*\\\\d+ (?:minute|min\\\\.?|hour|hr\\\\.?|day|week|wk\\\\.?|month|mo\\\\.?|year|yr\\\\.?)s?)\", \n",
    "                       secIDs = [\"brief_summary\", \"detailed_description\"], \n",
    "                       n = 10, \n",
    "                       seed = 42,\n",
    "                       index = compounded,\n",
    "                       root_dir = \"../clinicaltrials_data/trials/\"\n",
    "                      ):\n",
    "    secIDs = re.compile(\"|\".join(secIDs))\n",
    "    sentIDs = filter(lambda x: secIDs.search(x), index.get(group, {}).get(indicator, []))\n",
    "    random.seed(seed)\n",
    "    lines = []\n",
    "    \n",
    "    if len(sentIDs) > n:\n",
    "        sentIDs = random.sample(sentIDs, n)\n",
    "    for sentID in sentIDs:\n",
    "        IDparts = re.split(\"\\.\", sentID)\n",
    "        textblocks = {}\n",
    "        textblocks = extract_textblocks(IDparts[0], root_dir = root_dir)\n",
    "        line = nltk.sent_tokenize(textblocks[IDparts[1]])[int(IDparts[2])]\n",
    "        if type(line) == str:\n",
    "            line = unicode(line, \"utf-8\")\n",
    "        lines.append(line)\n",
    "\n",
    "    return lines, sentIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set `seed` to 0 and retrieve 10 sentences containing matches with the temporal indicator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, _ = retrieve_sentences(seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Functional biomechanical outcomes will be measured at 6 months and 12 months using DSX at the Biodynamics Lab.',\n",
       " u'This task takes about 30 minutes to complete.',\n",
       " u'Despite the challenging perspective of the new antiviral drugs directly acting on hepatitis C viral replication such as protease and polymerase inhibitors, nowadays the standard treatment in genotype 1-chronic hepatitis C (CHC) is the combination of peghylated interferon (PEG-IFN) and ribavirin for 48 weeks.',\n",
       " u'These last set of questionnaires should take about 30 minutes to complete.',\n",
       " u'Patients who were discharged after an uneventful ERCP were contacted by telephone within 5 days to capture delayed occurrence of the primary end point.',\n",
       " u'The study will include 80 children in ages 9-12 years, devided in 2 groups; children with ADHD treated with Methylphenidate, and healthy children without ADHD.',\n",
       " u'Course of treatment\\uff1a10 days.',\n",
       " u\"After completion of the study intervention, patients' medical charts are reviewed periodically for up to 2 years.\",\n",
       " u'Prospective studies are needed to analyze the correlation between UPCR and proteinuria of 24 hours to assess response to therapy.',\n",
       " u'A prospective randomised, double-blind, crossover trial, comparing the effects of paracetamol 1g (500mg x2) four times daily with matched placebo on ambulatory and clinic BP, each given for 14 days, with a 14-day washout period.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using grammar and syntax\n",
    "\n",
    "Now, we'll use the `spacy` language engine to transform the sentence into a set of tokens containing helpful properties such as syntactic dependency, part of speech, and parent-child relations. We'll create a function that can print this output for any sentence passed to it called `print_parse()`. This function transforms a sentence into a list of `spacy` _tokens_ using the `spacy` NLP engine. Then, it prints out the token's index, its text, its dependency relation to its parent token, its part of speech, the text of its parent token, the part of speech of its parent token, and the list of its child tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parse(sentence):\n",
    "    sentence = unicode(sentence)\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.i, token.text, token.dep_, token.pos_, token.head.text, token.head.pos_,\n",
    "             [child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, say we want to explore the syntax tree of the sentence \"The quick brown fox jumps over the lazy dog.\" We call the `print_parse` function on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'The', u'det', u'DET', u'fox', u'NOUN', [])\n",
      "(1, u'quick', u'amod', u'ADJ', u'fox', u'NOUN', [])\n",
      "(2, u'brown', u'amod', u'ADJ', u'fox', u'NOUN', [])\n",
      "(3, u'fox', u'nsubj', u'NOUN', u'jumps', u'VERB', [The, quick, brown])\n",
      "(4, u'jumps', u'ROOT', u'VERB', u'jumps', u'VERB', [fox, over, .])\n",
      "(5, u'over', u'prep', u'ADP', u'jumps', u'VERB', [dog])\n",
      "(6, u'the', u'det', u'DET', u'dog', u'NOUN', [])\n",
      "(7, u'lazy', u'amod', u'ADJ', u'dog', u'NOUN', [])\n",
      "(8, u'dog', u'pobj', u'NOUN', u'over', u'ADP', [the, lazy])\n",
      "(9, u'.', u'punct', u'PUNCT', u'jumps', u'VERB', [])\n"
     ]
    }
   ],
   "source": [
    "print_parse(\"The quick brown fox jumps over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we use these helpful syntactic features to extract temporal information?\n",
    "\n",
    "We'll define two functions, `extract_temporal` and `find_start`. The extraction function is the high-level implementation of our vision to extract a readable fragment of the sentence containing the temporal pattern. The `find_start` function helps initiaite the tree navigation in `extract_temporal`. \n",
    "\n",
    "The general idea is to start at the \"root\" token of the match (e.g. in a sentence like \"Follow-up visits will occur at 6 and 12 months\", the root is the word \"months\".) and collect the tokens that are in its subtree, then navigate upwards from this root to the root or verb of the sentence or clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_start(pieces, sentence):\n",
    "    for piece in pieces:\n",
    "        for i in range(len(sentence) - len(piece) + 1):\n",
    "            j = i + len(piece)\n",
    "            segment = sentence[i:j]\n",
    "            ancestors = set()\n",
    "            if tuple([token.text for token in segment]) == piece:\n",
    "                for token in segment:\n",
    "                    token_ancestors = set([token])\n",
    "                    token_ancestors = token_ancestors.union(token.ancestors)\n",
    "                    if ancestors:\n",
    "                        ancestors = ancestors.intersection(token_ancestors)\n",
    "                    else:\n",
    "                        ancestors = ancestors.union(token_ancestors)\n",
    "                for token in ancestors:\n",
    "                    if not set(token.children).intersection(ancestors):\n",
    "                        yield token, segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal(sentence, matches):\n",
    "    sentence = unicode(sentence)\n",
    "    sentence = nlp(sentence)\n",
    "    indicator_tokens = set()\n",
    "    for match in matches:\n",
    "        indicator_tokens.add(tuple([token.text for token in nlp(unicode(match))]))\n",
    "    fragment = set()\n",
    "    for start, segment in find_start(indicator_tokens, sentence):\n",
    "        head = start\n",
    "        for token in head.subtree:\n",
    "            fragment.add(token)\n",
    "        while head.pos_ != \"VERB\" and head.dep_ != \"ROOT\":\n",
    "            head = head.head\n",
    "            fragment.add(head)\n",
    "    return sorted(fragment, key = lambda x: x.i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out using our massive temporal indicator with the sentences retrieved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator = \"(?:(?:minute|min\\\\.?|hour|hr\\\\.?|day|week|wk\\\\.?|month|mo\\\\.?|year|yr\\\\.?)s? (?:\\\\d+-|\\\\d+\\\\.)*\\\\d+|(?:\\\\d+-|\\\\d+\\\\.)*\\\\d+ (?:minute|min\\\\.?|hour|hr\\\\.?|day|week|wk\\\\.?|month|mo\\\\.?|year|yr\\\\.?)s?)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Functional biomechanical outcomes will be measured at 6 months and 12 months using DSX at the Biodynamics Lab.\n",
      "Extract:  [measured, at, 6, months, and, 12, months]\n",
      "\n",
      "Sentence:  This task takes about 30 minutes to complete.\n",
      "Extract:  [takes, about, 30, minutes]\n",
      "\n",
      "Sentence:  Despite the challenging perspective of the new antiviral drugs directly acting on hepatitis C viral replication such as protease and polymerase inhibitors, nowadays the standard treatment in genotype 1-chronic hepatitis C (CHC) is the combination of peghylated interferon (PEG-IFN) and ribavirin for 48 weeks.\n",
      "Extract:  [ribavirin, for, 48, weeks]\n",
      "\n",
      "Sentence:  These last set of questionnaires should take about 30 minutes to complete.\n",
      "Extract:  [take, about, 30, minutes]\n",
      "\n",
      "Sentence:  Patients who were discharged after an uneventful ERCP were contacted by telephone within 5 days to capture delayed occurrence of the primary end point.\n",
      "Extract:  [contacted, within, 5, days]\n",
      "\n",
      "Sentence:  The study will include 80 children in ages 9-12 years, devided in 2 groups; children with ADHD treated with Methylphenidate, and healthy children without ADHD.\n",
      "Extract:  [include, 9, -, 12, years, ,, devided, in, 2, groups, ;]\n",
      "\n",
      "Sentence:  Course of treatment：10 days.\n",
      "Extract:  []\n",
      "\n",
      "Sentence:  After completion of the study intervention, patients' medical charts are reviewed periodically for up to 2 years.\n",
      "Extract:  [reviewed, for, up, to, 2, years]\n",
      "\n",
      "Sentence:  Prospective studies are needed to analyze the correlation between UPCR and proteinuria of 24 hours to assess response to therapy.\n",
      "Extract:  [analyze, correlation, between, UPCR, of, 24, hours]\n",
      "\n",
      "Sentence:  A prospective randomised, double-blind, crossover trial, comparing the effects of paracetamol 1g (500mg x2) four times daily with matched placebo on ambulatory and clinic BP, each given for 14 days, with a 14-day washout period.\n",
      "Extract:  [given, for, 14, days]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    print \"Sentence: \", example\n",
    "    print \"Extract: \", extract_temporal(example, re.compile(indicator).findall(example))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does this output achieve our goals?\n",
    "\n",
    "While most of the extracts are readable sentence fragments, there are two problems with this output. \n",
    "\n",
    "First, we are chopping out parts of the sentences to make digesting the information easier for the user, but more often that not, fragments like \"given for 14 days\" leave the user with a desire for more information. What is given for 14 days? Further, this pipeline cannot ensure that it will present the user with information about the trial as a whole rather than a small and possibly less-important fragment.\n",
    "\n",
    "Second, if we keep grabbing larger and larger portions of the original sentence, this doesn't result in reduced complexity for the reader at all. This doesn't get us any closer to our stated goal of improving readability.\n",
    "\n",
    "#### So, where can we go from here?\n",
    "Unfortunately, these fragments are often not quite appropriate for patients as simplified text. However, they are temporal extracts describing the scheduling topic! This could be handy when we get into our simplification moonshot. The style-transfer algorithm we're experimenting with requires topic-specific text for training, and this tool gets us text on the scheduling topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick-and-dirty exercise\n",
    "\n",
    "We conduct a separate experiment in extracting temporal information. What if we made the simple assumption that the largest time period mentioned in a summary or description section is likely the approximate duration of the trial?\n",
    "\n",
    "All we would need to do is extract the temporal pattern matches and perform some simple arithmetic calculation to determine which fragment denotes the largest amount of time.\n",
    "\n",
    "First, we define a function, `extract_time_strings`, that returns the temporal patterns found in a textblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_strings(textblock):\n",
    "    indicator = re.compile(\"(?:(?:minute|min\\\\.?|hour|hr\\\\.?|day|week|wk\\\\.?|month|mo\\\\.?|year|yr\\\\.?)s? (?:\\\\d+-|\\\\d+\\\\.)*\\\\d+|(?:\\\\d+-|\\\\d+\\\\.)*\\\\d+ (?:minute|min\\\\.?|hour|hr\\\\.?|day|week|wk\\\\.?|month|mo\\\\.?|year|yr\\\\.?)s?)\")\n",
    "    sentences = nltk.sent_tokenize(textblock)\n",
    "    time_strings = []\n",
    "    for sentence in sentences:\n",
    "        padded = \" \" + sentence.lower().strip() + \" \"\n",
    "        if indicator.search(padded):\n",
    "            nuggets = indicator.findall(padded)\n",
    "            time_strings.extend(nuggets)\n",
    "    return time_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a `convert_to_time` function that allows us to compare the different time strings. We'll need some convenient multipliers to perform this calculation.\n",
    "\n",
    "Just for fun, we derive our multipliers from two constants: the duration of a [stellar day](https://en.wiktionary.org/wiki/stellar_day) in seconds, and the number of stellar days in a year (366.2422). This saves us from dealing with leap years and arbitrary month-lengths!\n",
    "\n",
    "We'll need to handle some contingencies such as reverse-order time strings and ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "multipliers = {\n",
    "    \"minutes\" :       59.8362,\n",
    "    \"hours\"   :     3590.1708,\n",
    "    \"days\"    :    86164.0989,\n",
    "    \"weeks\"   :   603148.6923,\n",
    "    \"months\"  :  2629744.0953,\n",
    "    \"years\"   : 31556929.1435\n",
    "}\n",
    "\n",
    "def convert_to_time(time_strings):\n",
    "    times_and_strings = []\n",
    "    for time_string in time_strings:\n",
    "        if re.compile(r\"\\d+\").search(time_string.split()[1]): # handling reverse order time strings\n",
    "            unit, num = time_string.split()\n",
    "        else:\n",
    "            num, unit = time_string.split()\n",
    "        if len(num.split(\"-\")) > 1: # handling ranges\n",
    "            num = float(num.split(\"-\")[-1])\n",
    "        else:    \n",
    "            num = float(num)\n",
    "        if unit[-1] != \"s\":\n",
    "            unit += \"s\"\n",
    "        re.sub(\"mins\", \"minutes\", unit)\n",
    "        re.sub(\"hrs\", \"hours\", unit)\n",
    "        re.sub(\"wks\", \"weeks\", unit)\n",
    "        re.sub(\"mos\", \"months\", unit)\n",
    "        re.sub(\"yrs\", \"years\", unit)\n",
    "        multiplier = multipliers[unit]\n",
    "        time = num * multiplier\n",
    "        times_and_strings.append([time, time_string])\n",
    "    return times_and_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the high-level function `find_largest_time` to encapsulate all the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_largest_time(times_and_strings):\n",
    "    sorted_times_and_strings = sorted(times_and_strings, reverse = True)\n",
    "    if sorted_times_and_strings:\n",
    "        return sorted_times_and_strings[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect some blocks of texts from a set of trials and test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_ids = [\n",
    "    'NCT01610479',\n",
    "    'NCT00817739',\n",
    "    'NCT01915667',\n",
    "    'NCT01784029',\n",
    "    'NCT00003097',\n",
    "    'NCT02689219',\n",
    "    'NCT03311841',\n",
    "    'NCT01599000',\n",
    "    'NCT02412254',\n",
    "    'NCT03356236',\n",
    "    'NCT03187249',\n",
    "    'NCT03129113',\n",
    "    'NCT01038180',\n",
    "    'NCT02668172',\n",
    "    'NCT00586092',\n",
    "    'NCT00099502',\n",
    "    'NCT00138710',\n",
    "    'NCT02592863',\n",
    "    'NCT02683902',\n",
    "    'NCT00009048'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_of_text = []\n",
    "for trial_id in trial_ids:\n",
    "    textblocks = extract_textblocks(trial_id)\n",
    "    for section in textblocks.keys():\n",
    "        if section == \"brief_summary\" or section == \"detailed_description\":\n",
    "            blocks_of_text.append(textblocks[section])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this randomised, sham-controlled trial, investigators will recruit forty patients with primary cranial-cervical dystonia to receive an implanted device for STN-DBS, and participants will be randomly assigned to receive either neurostimulation or sham stimulation for 3 months.The primary end point was the change from baseline to 3 months in the severity of symptoms, according to the Burke-Fahn-Marsden Dystonia Rating Scale. Two masked dystonia experts who unaware of treatment status will assess the severity of dystonia by reviewing standardised videos.Subsequently, all patients will receive open-label neurostimulation; blinded assessment will be repeated after 6 months of active treatment.\n",
      "\n",
      "*** LARGEST TIME: 6 months ***\n",
      "\n",
      "The present study is being undertaken to compare counterregulatory hormone responses to a mild and gradual reduction in plasma glucose in young children with T1DM versus responses in adolescents. The studies will be performed under the close supervision of the professional staff of each DirecNet center and frequent bedside monitoring of plasma glucose concentrations will ensure that clinically significant hypoglycemia is prevented from developing. All subjects will be admitted to the CRC and have an IV line for blood sampling inserted on the evening prior to study to reduce stress on the morning of the study. The study procedure will be simplified and made less invasive in comparison to a clamp or standard insulin infusion study (i.e. only the one IV for blood sampling will be needed) by limiting enrollment to insulin pump-treated subjects who will have their basal rates modestly increased to produce the hypoglycemic stimulus. Monsod and colleagues used the same procedure of increasing the basal insulin infusion dose to induce a gradual fall in plasma glucose in youth with type 1 diabetes in a study that compared the ability of injections of glucagon and epinephrine to treat mild hypoglycemia. It is particularly important to note that once the blood glucose level falls below 60 mg/dl, a blood sample will be obtained and hypoglycemia will then be immediately corrected by intravenous administration of exogenous glucose. In our recent DirecNet study, ~25% of children and adolescents had plasma glucose levels below 60 mg/dl during a typical night and this rose to ~50% of subjects when there was antecedent exercise in the late afternoon. Moreover, the frequency of mild as well as severe hypoglycemia is substantially higher in pre-school children than in children and adolescents. This safe and rigorously designed study will provide important new information regarding the role of inadequate counter-regulation on the increased risk of hypoglycemia in very young children with T1DM. Real-time continuous glucose sensing systems offer the potential to markedly lower the risk of hypoglycemia in youth with T1DM. However, the DirecNet inpatient accuracy study demonstrated that the first generation of these devices was inaccurate when blood glucose was lowered to less than 70 mg/dl. In that study, children with T1DM between 3-17 years of age were admitted to the CRC for approximately 26 hours during which they wore 1-2 Medtronic MiniMed CGMS and 1-2 Cygnus GlucoWatch G2 Biographer continuous glucose monitors. In every subject in that study, blood samples were obtained every 30-60 minutes from an indwelling intravenous catheter for measurement of reference plasma glucose levels in the DirecNet Central Laboratory. The Guardian-RT continuous glucose monitoring systems is a real-time continuous glucose monitor that has considerable promise for use in children with diabetes. Therefore, a secondary aim of this study is to obtain very important data regarding the accuracy of this system during hypoglycemia in young children, as well as adolescents. The Guardian-RT has been approved by the FDA for detecting trends and tracking patterns in adults (18 and older) and are indicated for adjunctive rather than replacement of standard home glucose monitoring devices. The sensor has been approved by the FDA for use for up to 72 hours but can function for a longer period of time. The primary objective of this study will be to compare the glucose level at which counter-regulatory hormone responses occur during hypoglycemia in young children with diabetes, with the glucose level counter regulatory hormone responses that occur in older children with diabetes. We hypothesize that the children in the younger age group will not have a counterregulatory response until a lower glucose level is reached compared with children in the older age group. Secondary objectives will be:. 1. To assess signs and symptoms at different glucose levels of hypoglycemia in younger children and compare those with the older children. Furthermore, symptoms will be compared to counter regulatory hormone levels in the two age groups. Symptoms will be assessed using physiologic data and using an age-appropriate questionnaire that would be completed during the test by the subject (where appropriate) and by a parent. 2. To assess whether there is a difference in counter-regulatory hormone response in each age group, among those who had at least 2-3 episodes of hypoglycemia per day or night prior to the study compared with those who had an occasional or no episodes of hypoglycemia prior to the test. Subjects will wear a Guardian RT for 6 days (+1 day) prior to the test to assess episodes of hypoglycemia. 3. To examine the accuracy of the Guardian-RT during hypoglycemia in children with type 1 diabetes. Beginning the Study. When a child enters the study, the following will be done:. 1. Informed consent is obtained from eligible subjects (age 3 to <7 or 12 to <18 years, T1D for >1 year, insulin pump being used). 2. On the day of enrollment a hemoglobin A1c is obtained and instructions are given for use of the Guardian RT. The study personnel will supervise the subject or parent inserting the sensor in the clinic. The subject will be instructed to complete at least four glucose measurements a day using the study HGM. Instructions will also be given for response to Guardian RT alarms prior to the CRC admission. 3. The subject will return for an 18-hour overnight CRC admission approximately 6 days (+ 1 day) after the enrollment visit. - Subjects will continue using the Guardian RT sensor inserted prior to the admission. - For subjects of sufficient size to accommodate additional devices, a second Guardian-RT sensor will be inserted. An intravenous catheter will be inserted for reference measurements (glucose, epinephrine, norepinephrine, cortisol, glucagon and GH), which will be drawn during the subcutaneous insulin infusion test the following morning to send to a central laboratory. - For subjects of sufficient weight (subjects >14.9kg at reinfusion centers and >26.3kg at discard centers) to accommodate the volume of blood required, blood glucose measurements will be made every 30 minutes during the admission to allow for assessment of the accuracy of the Guardian-RT. - For subjects of sufficient weight to accommodate the volume of blood required, blood glucose measurements will be made every 15 minutes for two hours after dinner. This will allow for assessment of the accuracy of the Guardian-RT in detecting change during a period of rising blood glucose. - At approximately 8:00 a.m. the subcutaneous insulin infusion test will start. - Prior to starting the test, a HypoMon® may be placed around the subject's chest using an adjustable strap. - The glucose concentration as measured by the study HGM must be >110 mg/dL to start the test. - At the start of the test, the basal insulin rate will be increased by approximately 25-50% to provide a gradual decline in blood glucose. A small priming bolus dose of insulin equal to approximately one hour of the subject's usual basal dose may also be given at the discretion of the investigator in addition to the 25-50% increase in the basal insulin. - The basal insulin rate may be increased an additional amount and additional bolus insulin doses may be given at the discretion of the investigator in order to get a gradual decline in the glucose concentration. - Blood samples will be collected for the laboratory and glucose will be checked with the study HGM with venous blood every 15 minutes until the glucose level reaches 100 mg/dL. Thereafter, the study HGM will be used to check the glucose levels with venous blood every 5-10 minutes depending on the rate of fall of the glucose level until the end of the study. - Blood samples will be collected for laboratory determination of hormone concentrations at baseline (before increasing the insulin infusion) and when the glucose levels are <90, <80, <70, and <60 mg/dL. - Subjects (or parent if appropriate) will be asked questions regarding symptoms of hypoglycemia each time the glucose level is checked with the study HGM. - Once the endpoint is reached (the first time the glucose is <60 mg/dL using the study HGM), the basal rate will be returned to normal, the subjects will be treated with intravenous glucose and breakfast will be provided. 4. Prior to discharge, the sensors will be removed and downloaded and the subject's insulin pump will be downloaded if possible. About 6 days (+1 day) following the enrollment visit, subjects will have an inpatient CRC admission of approximately 18 hours. Subjects will be admitted at approximately 3:00 PM to allow sufficient time to calibrate the sensors before dinner is provided. - Areas where a Guardian RT sensor was worn during the first week will be assessed by study personnel for any skin irritation. - The Guardian RT, HGM, and pump data from the previous week will be reviewed and changes will be made to diabetes management as needed. - Subjects will continue using the Guardian RT sensor last inserted at home. If the sensor is not functioning properly, a new sensor will be inserted. - For subjects of sufficient size, an additional Guardian-RT sensor will be inserted and calibrated approximately two hours later. An intravenous catheter will be inserted in an arm vein for collection of blood samples during the admission. The area where the catheter will be inserted may be numbed with Elamax or EMLA cream prior to catheter insertion. Once the study endpoint is reached (the first time the glucose is <60 mg/dL using the study HGM), the basal rate will be returned to normal and the subjects will be treated with intravenous glucose. An additional blood sample will be collected for laboratory determination of glucose and hormone concentrations 15 minutes following the treatment with intravenous glucose. Subjects will then be given breakfast and discharged. Prior to discharge, the sensor(s) will be removed and downloaded. The subject's insulin pump will be downloaded if possible.\n",
      "\n",
      "*** LARGEST TIME: 18 years ***\n",
      "\n",
      "The study is a 2-stage, double-blind, randomized, placebo-controlled study following a two-stage, statistical selection theory design. Fifty-six HIV positive subjects will be randomized onto Stage 1 that will comprise a 4-arm parallel group (one placebo and 3 treatment groups) trial. One or possibly two interim analyses will be performed to determine continuation to Stage 2. A blinded interim analysis to determine the superior active treatment arm of Stage 1 will be continued to Stage 2 after 8 subjects per arm have completed the 24-week dosing regimen and the interim analysis. The study will be terminated if the interim analysis identifies either significant safety issues, or demonstrable non-significance. Following a significant outcome in the blinded interim analysis, the selected active and placebo control arms will continue blinded until total n=48 participants per arm for the placebo and selected treatment group have completed 24 weeks per arm. Respective groups will receive capsules containing L. frutescens in dosages of 0 (placebo material), 400mg bid, 800 mg bid or 1200 mg bid in the first stage. Progression to stage 2 will utilize a two arm design in which 34 subjects will receive either 0 mg L. frutescens (placebo) or the active dosage of L. frutescens bid for 24 weeks. Primary objectives are to determine the safety of Lessertia frutescens when used by HIV-1 infected adults with early disease, and to document the impact of Lessertia frutescens on markers of HIV disease progression. Secondary objective is to determine the effect of Lessertia frutescens on quality of life in HIV-infected adults and length of infection.\n",
      "\n",
      "*** LARGEST TIME: 24 weeks ***\n",
      "\n",
      "The aim of the study is to assess the incidence of patients suffering from ischemic stroke or transient ischemic attacks who have underlying asymptomatic paroxysmal atrial fibrillation. Patients who have suffered an ischemic stroke or transient ischemic attack, without a history of atrial fibrillation, are planned to be included. Starting within 14 days of the ischemic stroke, participating patients are asked to perform 10 second ECG recordings using a handheld ECG device twice daily (mornings and evenings) during 30 days. These recordings are transmitted via telephone to a secure encrypted Internet site. Within these 30 days the participants also perform an ambulatory 24 hour Holter recording. Handheld ECG recordings are evaluated continuously. In case of atrial fibrillation the patient is informed and offered treatment with anti coagulant medication (Warfarin). The investigation is a comparison between 24 hour continuous ECG recordings and short intermittent ECG recordings twice daily over a longer time period to determine which method is the best to detect atrial fibrillation in this patient group. Hypothesis: Short Intermittent ECG recordings over a longer time period is more efficient, compared with continuous 24 hour ECG recordings, in detecting silent paroxysmal AF in patients with an ischemic stroke and without a history of atrial fibrillation.\n",
      "\n",
      "*** LARGEST TIME: 30 days ***\n",
      "\n",
      "This prospective, single-arm clinical multicenter study intends to measure normal structural parameters of the optic nerve head, the peripapillary retinal nerve fiber layer, and the macula using the Heidelberg Spectralis OCT device. This study is conducted in Hispanic descent volunteers. The main goal of the study is to provide the range of these structural parameters in normal eyes. The study will include at least 240 normal volunteers; up to 10 study sites will recruit subjects. Overall an approximately equal age distribution from 18 to 90 years and an approximately equal number of females and males will be enrolled (approximately 40-60% females in each age group). All subjects will undergo Spectralis OCT imaging, biometric and ophthalmoscopic examination, disc photography and visual field testing in one single visit, if possible. Repeated perimetry or Spectralis scans may be performed at a second visit within 30 days of the initial visit. All examinations performed on the subjects are non-significant risk procedures.\n",
      "\n",
      "*** LARGEST TIME: 90 years ***\n",
      "\n",
      "Parallel, randomized and controlled clinical trial to evaluate the effect of 12 months of noninvasive mechanical ventilation versus conventional treatment in hypercapnic patients with stable COPD. Main objective: To evaluate the effect of 12 months of noninvasive ventilation on c-reactive protein concentration and daily physical activity in hypercapnic patients with stable COPD. Secondary objectives: To compare the plasmatic concentration of other inflammatory biomarkers between COPD patients with conventional treatment and wich noninvasive ventilation. To determine the response of breathlessness, health-related quality of life and lung function to noninvasive ventilation. To identify the COPD patients with a higher gasometric and clinic response to noninvasive ventilation.\n",
      "\n",
      "*** LARGEST TIME: 12 months ***\n",
      "\n",
      "The advance of communication technology has changed medical practice. The concept of medical telemedicine centers receiving, assessing and managing calls from patients or their carers on a 24 hour, 7 days a week basis is spreading in the US as well as in European countries. Usually specially trained nurses using dedicated decision support software provide the service. One aspect is the so-called triage, i.e. the decision on urgency and optimal treatment path of a stated medical problem, which has the potential to contain overall treatment costs. Despite proliferation of these services, little is known about the quality of the services provided. In October 2000 the first 24-hour medical telemedicine centre staffed by medical doctors started operating in Switzerland. The aims of this study are to assess the quality of service provided by doctors using decision support software and to determine potential predictors of incorrect triage.\n",
      "\n",
      "*** LARGEST TIME: 7 days ***\n",
      "\n",
      "The main objective of this Phase 2, multicenter, open-label study is to evaluate the safety and efficacy of different palovarotene dosing regimens in subjects with FOP. In Part A, all subjects who completed Study PVO-1A-201 and enrolled into the current study received daily treatment with open-label palovarotene for an eligible flare-up at a dose of 10 mg for 14 days, followed by 5 mg for 28 days (or the weight-based equivalent). Part A is completed. In Part B, subjects with at least 90% skeletal maturity were treated with 5 mg palovarotene on a daily basis (ie, chronically). In the event of an eligible flare-up, all subjects received 20 mg palovarotene daily for 28 days, followed by 10 mg for 56 days (dosing was weight-based in subjects who were skeletally immature). Dosing could be extended if the flare-up was not resolved by Flare-up Day 84 and continued until the flare-up resolved. Dose reduction, as directed by the Investigator, occurred in the event of intolerable side effects. The duration of Part B is up to 24 months. In Part C, the dosing regimens implemented in Part B will continue except that subjects with less than 90% skeletal maturity will now receive chronic daily administration of palovarotene (5 mg, or the weight-based equivalent). The assessment of HO will occur every 12 months using low-dose, whole body computed tomography (WBCT), excluding head; other efficacy and safety outcomes will be evaluated remotely every 3 months, or monthly during flare-up based treatment. The duration of Part C is 36 months.\n",
      "\n",
      "*** LARGEST TIME: 36 months ***\n",
      "\n",
      "The primary objective of the trial is to compare, in patients presenting with ST elevation myocardial infarction (STEMI) and multi-vessel disease (MVD), the safety and efficacy of immediate complete primary percutaneous coronary intervention (PCI) of all target vessels versus culprit vessel only revascularization, followed by staged PCI (within minimal 19 and maximal 45 days) of all target vessels, in a non-inferiority trial, using a third generation biodegradable-polymer everolimus-eluting stent (Synergy™).\n",
      "\n",
      "*** LARGEST TIME: 45 days ***\n",
      "\n",
      "An investigator-initiated, randomized, multicenter, non-blinded study of consecutive patients presenting with STEMI and MVD in stable hemodynamic conditions, undergoing after successful PCI of the culprit lesion either (1:1 randomization) immediate revascularization during the index procedure of all additional target lesions or staged PCI of additional target lesions (within 19-45 days) using the Boston Scientific Synergy™ stent. The goal of this trial is to compare two treatment strategies that are currently performed in clinical practice: complete immediate revascularization versus staged revascularization in patients with STEMI with MVD, using in both groups the Synergy™ stent. Patients randomized to complete immediate revascularization will have treated during the index procedure, after the revascularization of the culprit lesion responsable for the STEMI, the other significant coronary stenosis. Patients randomized to staged revascularization will have treated during the index admission only the culprit lesion and they will be hospitalized again after 19-45 days to complete the revascularization on the other significant coronary lesions. For both groups lesion are defined significant when causing a ≥70% diameter stenosis by visual estimation in at least two projections, with current on line state of the art angiographic equipment of the participating centers.\n",
      "\n",
      "*** LARGEST TIME: 45 days ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for textblock in blocks_of_text:\n",
    "\n",
    "    largest_time = find_largest_time(convert_to_time(extract_time_strings(textblock)))\n",
    "    if largest_time:\n",
    "        print textblock\n",
    "        if re.compile(r\"\\-\").search(largest_time): # in case the largest time is part of a range\n",
    "            parts = largest_time.split(\"-\")\n",
    "            parts = sorted(parts, key = lambda x: len(x), reverse = True)\n",
    "            largest_time = parts[0]\n",
    "        print \"\\n*** LARGEST TIME: \" + largest_time + \" ***\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What can this simple algorithm do for us?\n",
    "\n",
    "By identifying the largest period of time mentioned in a text, it potentially gets us close to an extraction of total trial duration. \n",
    "While there is an abundance of false positives, these are often much like the false positives we're encountering in our `extract_temporal()` function. We'll deal with these in a separate experiment using some standard supervised machine learning. But aside from that, this largest-indicator strategy is limited by semantics. For example, we don't know if a temporal mention relates to e.g., an active vs. follow-up period. So, because we are providing a _summary_ of temporal information&mdash;the _largest_ time&mdash;we're losing information from timing.\n",
    "\n",
    "## Is this the best that rule-based extraction can do?\n",
    "Both of our rule-based extraction methods have a common drawback here. Because they both stop at _extracting_ morphological components (e.g., words from the descriptions), they are subject to the inconsistencies in the trials, while not being imbued with their full contexts. In other words, to make our rule-based ouput stronger by using more rule-based processing we'd have to write some very complex (and overfit) case-by-case rules. Folks in NLP probably won't be surprised by this, it's really the nature of rule-based systems. While they can have high precision, they can be labor intensive to construct while being easy to over-fit. Stay tuned for our next pass at the temporal information—since the target temporal data are numeric, we'll be building some regression and supervised machine learning tools to estimate durations and frequencies of visits, etcetera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
