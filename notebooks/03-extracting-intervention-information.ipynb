{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring NLP for Clinical Trial Records\n",
    "## Installment 3: Extracting Intervention-related Information\n",
    "### by Munif Mujib\n",
    "\n",
    "In this notebook, out strategy will be to utilize specific verbs as _indicators_. From a sentence's syntactical dependency tree, we'll follow the verb to collect sentence fragments that might contain potentially useful information related to interventions. We'll also develop a system to collect a list of verbs using proper nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting tools in place\n",
    "\n",
    "We'll import the necessary modules and previously defined functions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import xml.etree.ElementTree as ET\n",
    "import re, glob, time\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "import random\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_node(parent, parent_dict):\n",
    "    if list(parent):\n",
    "        for child in parent:\n",
    "            child_dict = {}\n",
    "            child_dict = process_node(child, child_dict)\n",
    "            parent_dict[child.tag] = child_dict\n",
    "    else:\n",
    "        parent_dict[\"field_value\"] = parent.text\n",
    "    return parent_dict\n",
    "\n",
    "def load_trial(trial_id, root_dir = \"../clinicaltrials_data/trials/\"):\n",
    "    directory = trial_id[:-4] + \"xxxx\"\n",
    "    filepath = root_dir + directory + \"/\" + trial_id + \".xml\"\n",
    "    \n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    root_dict = {}\n",
    "    return process_node(root, root_dict)\n",
    "\n",
    "def find_textblocks(field, textblocks = [], current = \"root\"):\n",
    "    subfields = field.keys()\n",
    "    if not subfields == [\"field_value\"]:\n",
    "        if \"textblock\" in subfields:\n",
    "            textblocks.append((current, field[\"textblock\"][\"field_value\"]))\n",
    "        if len(subfields) > 1:    \n",
    "            for subfield in subfields:\n",
    "                if not subfield == \"textblock\":\n",
    "                    textblocks = find_textblocks(field[subfield], textblocks, current = current + \">\" + subfield)\n",
    "    return textblocks\n",
    "\n",
    "def process_textblock(textblock):\n",
    "    utext = textblock\n",
    "    lines = re.sub(r\"([^\\n.!?\\\"])\\n\\n\",r\"\\1.\\n\\n\",utext)\n",
    "    lines = re.sub(\"\\n*[ ]+\", \" \", lines)\n",
    "    lines = lines.strip()\n",
    "    return lines\n",
    "\n",
    "def get_textblocks(textblocks):\n",
    "    textblocks_dict = {}\n",
    "    for textblock in textblocks:\n",
    "        levels = re.split(\">\", textblock[0])\n",
    "        key = \">\".join(levels[1:])\n",
    "        textblocks_dict[key] = process_textblock(textblock[1])\n",
    "    return textblocks_dict\n",
    "\n",
    "def extract_textblocks(trial_id, root_dir = \"../clinicaltrials_data/trials/\"):\n",
    "    return get_textblocks(find_textblocks(load_trial(trial_id, root_dir = root_dir), textblocks = []))\n",
    "\n",
    "def create_regex_format(x):\n",
    "    regex = re.compile(r\"[^a-z0-9](\" + x + r\")[^a-z0-9]\")\n",
    "    return regex\n",
    "    \n",
    "def find_indicators(element, indicators_dict = {}, current = \"root\"):\n",
    "    if type(element) == dict:\n",
    "        subelements = element.keys()\n",
    "        if \"indicators\" in subelements and element[\"indicators\"]:\n",
    "            indicators_dict[re.sub(\"refinements>\", \"\",current)] = (map(create_regex_format, element[\"indicators\"]))\n",
    "        if len(subelements) > 1:\n",
    "            for subelement in subelements:\n",
    "                if type(element[subelement]) == dict:\n",
    "                    indicators_dict = find_indicators(\n",
    "                        element[subelement], indicators_dict, current = current + \">\" + subelement\n",
    "                    )\n",
    "    return indicators_dict\n",
    "\n",
    "def match_indicators(d, indicators_dict, secID):\n",
    "    lines = d.get(secID,\"\")\n",
    "    found = defaultdict(list)\n",
    "    for sentNum, sentence in enumerate(nltk.sent_tokenize(lines)):\n",
    "        for group in indicators_dict:\n",
    "            matches = []\n",
    "            for n, indicator in enumerate(indicators_dict[group]):\n",
    "                padded = \" \" + sentence.lower().strip() + \" \"\n",
    "                if indicator.search(padded):\n",
    "                    nuggets = indicator.findall(padded)\n",
    "                    matches.append(re.sub(r\"^.*?\\]\\((.*?)\\)\\[.*?$\",r\"\\1\",indicator.pattern))\n",
    "            if matches:\n",
    "                found[group].append((matches, str(sentNum)))\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parse(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, token.dep_, token.pos_, token.head.text, token.head.pos_,\n",
    "             [child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also load our pre-generated _compound_ index of trial, section, and sentence IDs and the `retrieve_sentences` function, which helps us quickly access example sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounded = json.load(open(\"../compounded.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_sentences(group = \"root>burden\", \n",
    "                       indicator = \"(?:(?:minute|min\\\\.?|hour|hr\\\\.?|day|week|wk\\\\.?|month|mo\\\\.?|year|yr\\\\.?)s? (?:\\\\d+-|\\\\d+\\\\.)*\\\\d+|(?:\\\\d+-|\\\\d+\\\\.)*\\\\d+ (?:minute|min\\\\.?|hour|hr\\\\.?|day|week|wk\\\\.?|month|mo\\\\.?|year|yr\\\\.?)s?)\", \n",
    "                       secIDs = [\"brief_summary\", \"detailed_description\"], \n",
    "                       n = 10, \n",
    "                       seed = 42,\n",
    "                       index = compounded,\n",
    "                       root_dir = \"../clinicaltrials_data/trials/\"\n",
    "                      ):\n",
    "    secIDs = re.compile(\"|\".join(secIDs))\n",
    "    sentIDs = filter(lambda x: secIDs.search(x), index.get(group, {}).get(indicator, []))\n",
    "    random.seed(seed)\n",
    "    lines = []\n",
    "    \n",
    "    if len(sentIDs) > n:\n",
    "        sentIDs = random.sample(sentIDs, n)\n",
    "    for sentID in sentIDs:\n",
    "        IDparts = re.split(\"\\.\", sentID)\n",
    "        textblocks = {}\n",
    "        textblocks = extract_textblocks(IDparts[0], root_dir = root_dir)\n",
    "        line = nltk.sent_tokenize(textblocks[IDparts[1]])[int(IDparts[2])]\n",
    "        if type(line) == str:\n",
    "            line = unicode(line, \"utf-8\")\n",
    "        lines.append(line)\n",
    "\n",
    "    return lines, sentIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating the syntax tree\n",
    "\n",
    "First, we'll need the handy `find_start` function that helps us locate the root of an indicator inside a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_start(pieces, sentence):\n",
    "    for piece in pieces:\n",
    "        for i in range(len(sentence) - len(piece) + 1):\n",
    "            j = i + len(piece)\n",
    "            segment = sentence[i:j]\n",
    "            ancestors = set()\n",
    "            if tuple([token.text for token in segment]) == piece:\n",
    "                for token in segment:\n",
    "                    token_ancestors = set([token])\n",
    "                    token_ancestors = token_ancestors.union(token.ancestors)\n",
    "                    if ancestors:\n",
    "                        ancestors = ancestors.intersection(token_ancestors)\n",
    "                    else:\n",
    "                        ancestors = ancestors.union(token_ancestors)\n",
    "                for token in ancestors:\n",
    "                    if not set(token.children).intersection(ancestors):\n",
    "                        yield token, segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll define our extraction function, `extract_intervention`. As with our extraction function for temporal indicators (from Installment 2), this function is provided a sentence and a list of indicator-matched fragments as input arguments. In this case, our indicator-matches will simply be verbs defined as intervention-related indicators. Using the `find_start` function to identify the starting point of our tree navigation, we will collect the verb along with auxiliaries and negatives. Then we will collect the nouns in all subjects and direct objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_intervention(sentence, matches):\n",
    "    sentence = nlp(unicode(sentence))\n",
    "    indicator_tokens = set()\n",
    "    for match in matches:\n",
    "        indicator_tokens.add(tuple([token.text for token in nlp(unicode(match))]))\n",
    "    \n",
    "    for start, segment in find_start(indicator_tokens, sentence):\n",
    "        verb_fragment = set()\n",
    "        subj_fragments = []\n",
    "        obj_fragments = []\n",
    "        children = set()\n",
    "        subjs = []\n",
    "        objs = []\n",
    "        \n",
    "        head = start\n",
    "        verb_fragment.add(head)\n",
    "        \n",
    "        for child in head.children:\n",
    "            children.add(child)\n",
    "\n",
    "        while children:\n",
    "            child = children.pop()\n",
    "            if re.search(\"subj\", child.dep_):\n",
    "                subjs.append(child)\n",
    "            elif re.search(\"dobj\", child.dep_):\n",
    "                objs.append(child)\n",
    "            elif re.search(\"neg|aux\", child.dep_):\n",
    "                verb_fragment.add(child)\n",
    "            else:\n",
    "                for grandchild in child.children:\n",
    "                    children.add(grandchild)\n",
    "                    \n",
    "        verb_fragment = sorted(list(verb_fragment), key = lambda x: x.i)\n",
    "        \n",
    "        for subj in subjs:\n",
    "            subj_fragment = set()\n",
    "            for token in subj.subtree:\n",
    "                if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "                    subj_fragment.add(token)\n",
    "            parent = subj\n",
    "            while parent != head:\n",
    "                subj_fragment.add(parent)\n",
    "                parent = parent.head\n",
    "            subj_fragment = sorted(list(subj_fragment), key = lambda x: x.i)\n",
    "            subj_fragments.append(subj_fragment)\n",
    "        \n",
    "        for obj in objs:\n",
    "            obj_fragment = set()\n",
    "            for token in obj.subtree:\n",
    "                if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "                    obj_fragment.add(token)\n",
    "            parent = obj\n",
    "            while parent != head:\n",
    "                obj_fragment.add(parent)\n",
    "                parent = parent.head\n",
    "            obj_fragment = sorted(list(obj_fragment), key = lambda x: x.i)\n",
    "            obj_fragments.append(obj_fragment)\n",
    "            \n",
    "        fragment = [subj_fragments, verb_fragment, obj_fragments]\n",
    "        yield fragment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the function, we'll retrieve example sentences for the indicator \"receive\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, sentIDs = retrieve_sentences(group = \"root>burden>active period\", indicator = u\"receive\", n = 20, seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print our output in a readable format, we'll define a handy `print_intervention_output` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_intervention_output(sentence, verb):\n",
    "    print sentence\n",
    "    print \"\"\n",
    "    outputs = list(extract_intervention(sentence, [verb]))\n",
    "    \n",
    "    for output in outputs:\n",
    "        subject_string = \"\"\n",
    "        object_string = \" \"\n",
    "\n",
    "        if len(output[0]) > 1:\n",
    "            for subject in output[0]:\n",
    "                words = [token.text for token in subject]\n",
    "                single_subject_string = \" \".join(words)\n",
    "                subject_string = subject_string + single_subject_string + \", \"\n",
    "            subject_string = subject_string[:-2] + \" \"\n",
    "        elif len(output[0]) == 1:\n",
    "            subject_string += \" \".join([token.text for token in output[0][0]]) + \" \"\n",
    "\n",
    "        verb_string = \" \".join([token.text for token in output[1]])\n",
    "\n",
    "        if len(output[2]) > 1:\n",
    "            for objet in output[2]:\n",
    "                words = [token.text for token in objet]\n",
    "                single_object_string = \" \".join(words)\n",
    "                object_string = object_string + single_object_string + \", \"\n",
    "            object_string = object_string[:-2]\n",
    "        elif len(output[2]) == 1:\n",
    "            object_string = object_string + \" \".join([token.text for token in output[2][0]])\n",
    "\n",
    "        print subject_string + verb_string + object_string\n",
    "        print \"\"\n",
    "    print \"____________________________________________\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use all of this to examine some output from the test sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finally, group C, including 10 patients, will receive anti-VEGF in a different dose (injection volume) as compared to S1.\n",
      "\n",
      "group C patients will receive \n",
      "\n",
      "____________________________________________\n",
      "\n",
      "Arm I: Patients receive oral estramustine three times a day and oral etoposide twice daily on days 1-14 and paclitaxel IV over 1 hour on day 2.\n",
      "\n",
      "Patients receive estramustine, paclitaxel IV\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "In this study, all patients will receive chemotherapy and radiation therapy.\n",
      "\n",
      "patients will receive chemotherapy radiation therapy\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "Volunteers who provide samples for these studies will not routinely receive their individual results from the Additional Investigation.\n",
      "\n",
      "Volunteers who samples studies will not receive results\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "Further, women who received HOPE displayed fewer PTSD arousal and avoidance symptoms of PTSD, less depression, and greater social support and empowerment relative to women who did not receive HOPE.\n",
      "\n",
      "who did not receive HOPE\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "Participants in the GMT arm will receive counseling focused on medication adherence and smoking reduction plus 52 weeks of combined NRT.\n",
      "\n",
      "counseling focused, Participants GMT arm will receive \n",
      "\n",
      "____________________________________________\n",
      "\n",
      "All subjects will receive surgical treatment of their SCCis.\n",
      "\n",
      "subjects will receive treatment SCCis\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "Patients receive tacrolimus IV over 24 hours or orally daily on days -3 to 35 and oral mycophenolate mofetil twice daily on days -3 to 28 as graft-vs-host disease (GVHD) prophylaxis.\n",
      "\n",
      "Patients receive tacrolimus IV\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "Each group per Arm will receive the same dose of one of the three vaccines for a total of 60 subjects (15 subjects per dose-escalating Arm, 3 groups per Arm, 5 subjects per group).\n",
      "\n",
      "group Arm will receive dose vaccines total subjects subjects dose Arm groups Arm subjects group\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "Once the MTD has been determined, additional patients receive irinotecan at the dose level below the MTD with external beam radiation therapy on the same treatment schedule as above.\n",
      "\n",
      "patients, MTD determined has been receive irinotecan\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "Participants may receive up to 12 such cycles of daclizumab/denileukin diftitox therapy.\n",
      "\n",
      "Participants may receive cycles daclizumab denileukin diftitox therapy\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "Subjects on stable methadone maintenance therapy (20 - 120 mg daily as single oral morning dose) will receive danoprevir 100 mg orally twice daily and ritonavir 100 mg orally twice daily for 10 days.\n",
      "\n",
      "Subjects methadone maintenance therapy mg morning dose will receive danoprevir, mg, ritonavir mg\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "- Regimen 3: Patients receive cisplatin IV over 2 hours on day 1 and vinorelbine IV over 5 -10 minutes on days 1 and 8.\n",
      "\n",
      "Patients receive IV, vinorelbine IV\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "- Maintenance chemotherapy: Patients receive oral lomustine once on day 1 and cisplatin IV over 6 hours on day 1 and vincristine IV on days 1, 8, and 15.\n",
      "\n",
      "Patients receive lomustine\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "On Days 5 to 9, subjects receive twice-daily doses of ASP3652, and a single dose on the morning of Day 10.\n",
      "\n",
      "subjects receive doses ASP3652\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "Secondarily, we further hypothesized that students that receive the intervention will demonstrate higher knowledge about HPV and the HPV vaccine as compared to the control group and that African American students and younger students (defined as 18-19 years of age) would be less compliant as compared to Caucasian and older students (defined as students 20 and older), as demonstrated in the literature.\n",
      "\n",
      "that receive intervention\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "Patients will receive the same treatment throughout the study.\n",
      "\n",
      "Patients will receive treatment\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "Participants will also be informed about their fitness levels and will receive advice on how to improve these.\n",
      "\n",
      "will receive advice\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "The placebo group (n = 10): subjects that will receive intravenous infusion of NaCl 0.9% for 5 hours, starting 60 minutes before intravenous administration of 2 ng/kg LPS.\n",
      "\n",
      "that will receive infusion NaCl, starting minutes\n",
      "\n",
      "____________________________________________\n",
      "\n",
      "HIV-seronegative volunteers (including four populations at higher risk for HIV infection and two populations at lower risk) receive one of four regimens.\n",
      "\n",
      "HIV volunteers populations risk HIV infection populations risk receive one regimens\n",
      "\n",
      "____________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    print_intervention_output(example, \"receive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is this output useful to a patient?\n",
    "\n",
    "Some of the extracted sentence fragments are readable and provide concrete information that a potential participant in a trial may find useful. Sometimes, the output is broken, most often due to the difficulty of parsing complex sentences. Sometimes, it appears that the output simply does not contain enough information to be useful. Furthermore, these are still fragments of difficult-to-read sentences, and no real summarization or simplification is being performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting verbs\n",
    "\n",
    "Now, we'll cook up a system for going over trial records and counting up the verbs to populate a list of all verbs, which can potentially point us to indicators that might be useful (e.g., as input to the `extract_intervention` function).\n",
    "\n",
    "Our strategy will be to take a sentence and find any proper nouns in it. Then, we'll start navigating the tree upwards from this proper noun until we find the verb in the sentence that is associated with it. We'll collect the verb.\n",
    "\n",
    "To do all this, we'll write three functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_propns(tokens):\n",
    "    propn_locations = []\n",
    "    for token in tokens:\n",
    "        if token.pos_ == \"PROPN\":\n",
    "            propn_locations.append(token.i)\n",
    "    return propn_locations\n",
    "\n",
    "def collect_verb(sentence, ind):\n",
    "    token = sentence[ind]\n",
    "    while token.pos_ != \"VERB\" and token.dep_ != \"ROOT\":\n",
    "        token = token.head\n",
    "    if token.pos_ == \"VERB\":\n",
    "        return token\n",
    "    \n",
    "def find_propns_then_find_verbs(sentence):\n",
    "    sentence = nlp(unicode(sentence))\n",
    "    \n",
    "    propn_locs = find_propns(sentence)\n",
    "    \n",
    "    verbs = set()\n",
    "    for ind in propn_locs:\n",
    "        verb = collect_verb(sentence, ind)\n",
    "        if verb:\n",
    "            verbs.add(verb)\n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a __Counter__ object and a function to help populate the counter with verb counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_counter = Counter()\n",
    "\n",
    "def count_verbs(verbs):\n",
    "    if verbs:\n",
    "        for verb in verbs:\n",
    "            verb_counter[(verb.text).lower()] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll take a handful of random trial IDs and test this counting system on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_ids = [\n",
    "    'NCT01711775',\n",
    "    'NCT02308540',\n",
    "    'NCT00122642',\n",
    "    'NCT02407860',\n",
    "    'NCT00617630',\n",
    "    'NCT02587949',\n",
    "    'NCT03101254',\n",
    "    'NCT02621502',\n",
    "    'NCT00458601',\n",
    "    'NCT00476970',\n",
    "    'NCT00162292',\n",
    "    'NCT00708656',\n",
    "    'NCT02170350',\n",
    "    'NCT00463710',\n",
    "    'NCT01503853',\n",
    "    'NCT00001497',\n",
    "    'NCT02782039',\n",
    "    'NCT02108431',\n",
    "    'NCT03245203',\n",
    "    'NCT00001664',\n",
    "    'NCT00639028',\n",
    "    'NCT00381030',\n",
    "    'NCT02973100',\n",
    "    'NCT00528047',\n",
    "    'NCT00559702',\n",
    "    'NCT02339298',\n",
    "    'NCT02309437',\n",
    "    'NCT00496691',\n",
    "    'NCT01588275',\n",
    "    'NCT01928641',\n",
    "    'NCT01972854',\n",
    "    'NCT03089385',\n",
    "    'NCT01978704',\n",
    "    'NCT01205009',\n",
    "    'NCT03441022',\n",
    "    'NCT03405025',\n",
    "    'NCT03301636',\n",
    "    'NCT00793403',\n",
    "    'NCT02314390',\n",
    "    'NCT00942214',\n",
    "    'NCT00634946',\n",
    "    'NCT02929056',\n",
    "    'NCT03136302',\n",
    "    'NCT01712061',\n",
    "    'NCT02018705',\n",
    "    'NCT00954317',\n",
    "    'NCT03453814',\n",
    "    'NCT02998398',\n",
    "    'NCT01256736',\n",
    "    'NCT01678664',\n",
    "    'NCT03319732',\n",
    "    'NCT02269189',\n",
    "    'NCT02477332',\n",
    "    'NCT01821911',\n",
    "    'NCT02386787',\n",
    "    'NCT01289964',\n",
    "    'NCT01710163',\n",
    "    'NCT02867605',\n",
    "    'NCT01053676',\n",
    "    'NCT01679782',\n",
    "    'NCT00003509',\n",
    "    'NCT00042185',\n",
    "    'NCT02094092',\n",
    "    'NCT02344043',\n",
    "    'NCT01454414',\n",
    "    'NCT02998424',\n",
    "    'NCT03180424',\n",
    "    'NCT03266146',\n",
    "    'NCT00703352',\n",
    "    'NCT02614612',\n",
    "    'NCT00004446',\n",
    "    'NCT01212237',\n",
    "    'NCT02191007',\n",
    "    'NCT00561873',\n",
    "    'NCT00068081',\n",
    "    'NCT01820663',\n",
    "    'NCT02558868',\n",
    "    'NCT03249064',\n",
    "    'NCT00329355',\n",
    "    'NCT01113931',\n",
    "    'NCT01150032',\n",
    "    'NCT01324895',\n",
    "    'NCT03217539',\n",
    "    'NCT02278263',\n",
    "    'NCT03277742',\n",
    "    'NCT01690260',\n",
    "    'NCT03086889',\n",
    "    'NCT02068235',\n",
    "    'NCT00876733',\n",
    "    'NCT01951339',\n",
    "    'NCT03327272',\n",
    "    'NCT01635764',\n",
    "    'NCT00194415',\n",
    "    'NCT01839019',\n",
    "    'NCT02178423',\n",
    "    'NCT02229656',\n",
    "    'NCT00465205',\n",
    "    'NCT01392053',\n",
    "    'NCT01430598',\n",
    "    'NCT01689766'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_id in trial_ids:\n",
    "    textblocks = extract_textblocks(trial_id)\n",
    "    for sec in textblocks:\n",
    "        sentences = nltk.sent_tokenize(textblocks[sec])\n",
    "        for n, sentence in enumerate(sentences):\n",
    "            count_verbs(find_propns_then_find_verbs(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the counted verbs, sorted from most frequently occurring to least:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'is', 265),\n",
       " (u'are', 100),\n",
       " (u'treated', 75),\n",
       " (u'have', 75),\n",
       " (u'be', 75),\n",
       " (u'performed', 65),\n",
       " (u'evaluate', 65),\n",
       " (u'received', 60),\n",
       " (u'using', 55),\n",
       " (u'administered', 55),\n",
       " (u'used', 50),\n",
       " (u'receive', 45),\n",
       " (u'defined', 45),\n",
       " (u'compare', 40),\n",
       " (u'including', 40),\n",
       " (u'based', 40),\n",
       " (u'confirmed', 40),\n",
       " (u'assess', 40),\n",
       " (u'include', 35),\n",
       " (u'diagnosed', 35),\n",
       " (u'measured', 35),\n",
       " (u'assessed', 35),\n",
       " (u'considered', 35),\n",
       " (u'according', 35),\n",
       " (u'associated', 35),\n",
       " (u'identified', 30),\n",
       " (u'investigate', 30),\n",
       " (u'determine', 30),\n",
       " (u'compared', 30),\n",
       " (u'reported', 25),\n",
       " (u'followed', 25),\n",
       " (u'screening', 25),\n",
       " (u'provide', 25),\n",
       " (u'related', 25),\n",
       " (u'measure', 25),\n",
       " (u'receiving', 25),\n",
       " (u'count', 25),\n",
       " (u'taken', 20),\n",
       " (u'known', 20),\n",
       " (u'relapsing', 20),\n",
       " (u'found', 20),\n",
       " (u'conducted', 20),\n",
       " (u'determined', 20),\n",
       " (u'improve', 20),\n",
       " (u'evaluated', 20),\n",
       " (u'had', 20),\n",
       " (u'has', 20),\n",
       " (u'admitted', 20),\n",
       " (u'called', 20),\n",
       " (u'managed', 15),\n",
       " (u'prevent', 15),\n",
       " (u'sign', 15),\n",
       " (u'scan', 15),\n",
       " (u'started', 15),\n",
       " (u'following', 15),\n",
       " (u'allowed', 15),\n",
       " (u'documented', 15),\n",
       " (u'investigated', 15),\n",
       " (u'given', 15),\n",
       " (u'recruited', 15),\n",
       " (u'approved', 15),\n",
       " (u'show', 15),\n",
       " (u'judged', 15),\n",
       " (u'observed', 15),\n",
       " (u'see', 15),\n",
       " (u'speak', 15),\n",
       " (u'evaluating', 15),\n",
       " (u'continue', 15),\n",
       " (u'use', 15),\n",
       " (u'included', 15),\n",
       " (u'follow', 10),\n",
       " (u'appear', 10),\n",
       " (u'explore', 10),\n",
       " (u'published', 10),\n",
       " (u'meet', 10),\n",
       " (u'radiotherapy', 10),\n",
       " (u'undergoing', 10),\n",
       " (u'reduce', 10),\n",
       " (u'shown', 10),\n",
       " (u'certified', 10),\n",
       " (u'initiate', 10),\n",
       " (u'undergo', 10),\n",
       " (u'randomized', 10),\n",
       " (u'listed', 10),\n",
       " (u'participated', 10),\n",
       " (u'enhancing', 10),\n",
       " (u'completed', 10),\n",
       " (u'speaking', 10),\n",
       " (u'experienced', 10),\n",
       " (u'calculated', 10),\n",
       " (u'detect', 10),\n",
       " (u'reading', 10),\n",
       " (u'interfere', 10),\n",
       " (u'attributed', 10),\n",
       " (u'receives', 10),\n",
       " (u'puts', 10),\n",
       " (u'collected', 10),\n",
       " (u'observe', 10),\n",
       " (u'applied', 10),\n",
       " (u'taking', 10),\n",
       " (u'studied', 10),\n",
       " (u'develop', 10),\n",
       " (u'\\u2265', 10),\n",
       " (u'assigned', 10),\n",
       " (u'identify', 10),\n",
       " (u'regarded', 10),\n",
       " (u'scheduled', 10),\n",
       " (u'suffering', 10),\n",
       " (u'appearing', 10),\n",
       " (u'reduced', 10),\n",
       " (u'enrolling', 10),\n",
       " (u'was', 10),\n",
       " (u'complete', 10),\n",
       " (u'required', 10),\n",
       " (u'having', 10),\n",
       " (u'relieve', 5),\n",
       " (u'primed', 5),\n",
       " (u'leads', 5),\n",
       " (u'consists', 5),\n",
       " (u'discern', 5),\n",
       " (u'calculate', 5),\n",
       " (u'sitting', 5),\n",
       " (u'results', 5),\n",
       " (u'retested', 5),\n",
       " (u'induced', 5),\n",
       " (u'altering', 5),\n",
       " (u'overlaid', 5),\n",
       " (u'added', 5),\n",
       " (u'work', 5),\n",
       " (u'replicated', 5),\n",
       " (u'increasing', 5),\n",
       " (u'filled', 5),\n",
       " (u'deemed', 5),\n",
       " (u'communicate', 5),\n",
       " (u'focused', 5),\n",
       " (u'affiliated', 5),\n",
       " (u'sending', 5),\n",
       " (u'enroll', 5),\n",
       " (u'involved', 5),\n",
       " (u'study', 5),\n",
       " (u'changed', 5),\n",
       " (u'makes', 5),\n",
       " (u'involves', 5),\n",
       " (u'requiring', 5),\n",
       " (u'replace', 5),\n",
       " (u'decrease', 5),\n",
       " (u'rollover', 5),\n",
       " (u'undergone', 5),\n",
       " (u'prove', 5),\n",
       " (u'remains', 5),\n",
       " (u'distributing', 5),\n",
       " (u'administering', 5),\n",
       " (u'comorbid', 5),\n",
       " (u'occurred', 5),\n",
       " (u'tested', 5),\n",
       " (u'train', 5),\n",
       " (u'encountered', 5),\n",
       " (u'producing', 5),\n",
       " (u'der', 5),\n",
       " (u'averaged', 5),\n",
       " (u'remain', 5),\n",
       " (u'drops', 5),\n",
       " (u'indicated', 5),\n",
       " (u'give', 5),\n",
       " (u'indicates', 5),\n",
       " (u'want', 5),\n",
       " (u'divided', 5),\n",
       " (u'steepening', 5),\n",
       " (u'write', 5),\n",
       " (u'influences', 5),\n",
       " (u'fed', 5),\n",
       " (u'purchase', 5),\n",
       " (u'randomised', 5),\n",
       " (u'varieties', 5),\n",
       " (u'maintain', 5),\n",
       " (u'developed', 5),\n",
       " (u'vaccinated', 5),\n",
       " (u'satisfied', 5),\n",
       " (u'monitor', 5),\n",
       " (u'termed', 5),\n",
       " (u'overcome', 5),\n",
       " (u'decompensated', 5),\n",
       " (u'propose', 5),\n",
       " (u'introduce', 5),\n",
       " (u'maintaining', 5),\n",
       " (u'wash', 5),\n",
       " (u'identifies', 5),\n",
       " (u'repaired', 5),\n",
       " (u'characterized', 5),\n",
       " (u'referred', 5),\n",
       " (u'weighted', 5),\n",
       " (u'manifests', 5),\n",
       " (u'living', 5),\n",
       " (u'accomplish', 5),\n",
       " (u'increase', 5),\n",
       " (u'beginning', 5),\n",
       " (u'facilitate', 5),\n",
       " (u'stimulate', 5),\n",
       " (u'precluding', 5),\n",
       " (u'discharged', 5),\n",
       " (u'recruiting', 5),\n",
       " (u'compromise', 5),\n",
       " (u'recovered', 5),\n",
       " (u'took', 5),\n",
       " (u'recruit', 5),\n",
       " (u'aims', 5),\n",
       " (u'limiting', 5),\n",
       " (u'supposed', 5),\n",
       " (u'result', 5),\n",
       " (u'modulating', 5),\n",
       " (u'ras', 5),\n",
       " (u'need', 5),\n",
       " (u'aged', 5),\n",
       " (u'potentiated', 5),\n",
       " (u'jeopardize', 5),\n",
       " (u'instructed', 5),\n",
       " (u'causing', 5),\n",
       " (u'take', 5),\n",
       " (u'altered', 5),\n",
       " (u'precludes', 5),\n",
       " (u'gather', 5),\n",
       " (u'fitted', 5),\n",
       " (u'v600-activating', 5),\n",
       " (u'participate', 5),\n",
       " (u'promote', 5),\n",
       " (u'inserted', 5),\n",
       " (u'employed', 5),\n",
       " (u'achieve', 5),\n",
       " (u'means', 5),\n",
       " (u'facilitated', 5),\n",
       " (u'report', 5),\n",
       " (u'predicted', 5),\n",
       " (u'ascertain', 5),\n",
       " (u'respond', 5),\n",
       " (u'enhances', 5),\n",
       " (u'set', 5),\n",
       " (u'achieved', 5),\n",
       " (u'acquired', 5),\n",
       " (u'emulating', 5),\n",
       " (u'written', 5),\n",
       " (u'score', 5),\n",
       " (u'drawn', 5),\n",
       " (u'justified', 5),\n",
       " (u'extend', 5),\n",
       " (u'analyzed', 5),\n",
       " (u'suggested', 5),\n",
       " (u'invited', 5),\n",
       " (u'dl.', 5),\n",
       " (u'admit', 5),\n",
       " (u'swallow', 5),\n",
       " (u'asked', 5),\n",
       " (u'distinguish', 5),\n",
       " (u'considering', 5),\n",
       " (u'targeting', 5),\n",
       " (u'informed', 5),\n",
       " (u'been', 5),\n",
       " (u'combined', 5),\n",
       " (u'utilizing', 5),\n",
       " (u'define', 5),\n",
       " (u'kilograms', 5),\n",
       " (u'suppress', 5),\n",
       " (u'offered', 5),\n",
       " (u'lymphocytes', 5),\n",
       " (u'understand', 5),\n",
       " (u'suspected', 5),\n",
       " (u'developing', 5),\n",
       " (u'tolerate', 5),\n",
       " (u'irinotecan', 5),\n",
       " (u'promising', 5),\n",
       " (u'evaluates', 5),\n",
       " (u'perform', 5),\n",
       " (u'suggest', 5),\n",
       " (u'make', 5),\n",
       " (u'elevated', 5),\n",
       " (u'devised', 5),\n",
       " (u'evolve', 5),\n",
       " (u'permit', 5),\n",
       " (u'uses', 5),\n",
       " (u'diabetes', 5),\n",
       " (u'fasting', 5),\n",
       " (u'changes', 5),\n",
       " (u'proposed', 5),\n",
       " (u'being', 5),\n",
       " (u'reviewed', 5),\n",
       " (u'evidenced', 5),\n",
       " (u'tests', 5),\n",
       " (u'increased', 5),\n",
       " (u'kills', 5),\n",
       " (u'read', 5),\n",
       " (u'occurring', 5),\n",
       " (u'excluding', 5),\n",
       " (u'reduces', 5),\n",
       " (u'become', 5),\n",
       " (u'works', 5),\n",
       " (u'disorder', 5),\n",
       " (u'predisposing', 5),\n",
       " (u'specified', 5),\n",
       " (u'provided', 5),\n",
       " (u'confirm', 5),\n",
       " (u'affects', 5),\n",
       " (u'avoid', 5),\n",
       " (u'recorded', 5),\n",
       " (u'inject', 5),\n",
       " (u'demonstrated', 5),\n",
       " (u'pose', 5),\n",
       " (u'continuing', 5),\n",
       " (u'accompanied', 5),\n",
       " (u'refers', 5),\n",
       " (u'permitted', 5),\n",
       " (u'stage', 5),\n",
       " (u'gained', 5),\n",
       " (u'carried', 5),\n",
       " (u'completing', 5),\n",
       " (u'escalating', 5),\n",
       " (u'rising', 5),\n",
       " (u'reducing', 5),\n",
       " (u'qualifying', 5),\n",
       " (u'support', 5),\n",
       " (u'mutated', 5),\n",
       " (u'infected', 5),\n",
       " (u'touted', 5),\n",
       " (u'recommends', 5),\n",
       " (u'registered', 5),\n",
       " (u'repeated', 5),\n",
       " (u'sustained', 5),\n",
       " (u'removed', 5),\n",
       " (u'entered', 5),\n",
       " (u'directed', 5),\n",
       " (u'made', 5),\n",
       " (u'signed', 5),\n",
       " (u'placed', 5),\n",
       " (u'limit', 5),\n",
       " (u'contribute', 5),\n",
       " (u'proven', 5),\n",
       " (u'triglycerides', 5),\n",
       " (u'weaned', 5),\n",
       " (u'test', 5),\n",
       " (u'concerning', 5),\n",
       " (u'preceding', 5),\n",
       " (u'metabolized', 5),\n",
       " (u'utilised', 5),\n",
       " (u'presenting', 5),\n",
       " (u'enrolled', 5),\n",
       " (u'requires', 5)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summing up:\n",
    "\n",
    "While this is a slow process, it is exhaustive. We can count up all the verbs associated with proper nouns occurring across all of the nearly 260,000 trial records using this method. Reviewing the collected list of verbs, along with their frequencies, should yield some useful indicators for intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
